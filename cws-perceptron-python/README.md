##基于结构化平均感知器的中文分词

实现结构化感知器

最近因为准备接手师兄的个性化分词模块，同时希望在开学之前能多学一些东西，于是开始在感知器模型上做一些实事。

一件事是在博客上写的关于感知器的[文章](http://memeda.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2015/07/22/%E6%84%9F%E7%9F%A5%E5%99%A8%E5%AD%A6%E4%B9%A0.html)，主要包括感知器模型介绍，目标函数，更新算法。继而引出了一些感知器的变体，包括平均感知器，结构化感知器。*目前仍处于编辑状态。*

另一件事就是写出实际可用的代码。
**目标是快速开发，结构清晰。**

1. 选择Python实现

2. 不要求使用不熟悉的知识，不要求良好封装

3. 效果好

主要从[LTPCWS](https://github.com/HIT-SCIR/ltp-cws)中学习到相关处理细节：

1. 需要从训练集中提取到lexicon信息，并建立词典。

2. 中英文单词，URL，（数字）需要预处理 [暂不考虑]

3. 抽哪些特征
    
    实际抽的特征都是在输入词上抽的，与具体的标签无关。故可以认为这是emit特征（发射特征）。而转移特征可以认为是直接人为赋予的，如果是一阶马尔科夫就是前一个Label到当前label，如果是二阶，那么就是考虑前两个。

    对于emit特征，由于是针对输入字的，故与label无关。即对各label而言，emit特征可以认为是相同的；

    对于trans特征，其是在Viterbi解码时生成的。

    权值向量上，对于emit特征，每个类别都有相应的特征值，设emit特征数为N，那么需要的权值参数为N*label_num ；做一阶，则有label_num*label_num个权值参数，二阶有label_num^3个。

    一个问题是，这些权值参数该如何存储？ 是类似多元分类，为(N+label_num)*label_num的矩阵（仅对一阶有效，二阶可能需要把转移权值单独拿出来），还是把所有权值放在一起，作为一个向量？在LTPCWS中，是后者的处理方式，并使用了一个函数来映射同样的emit特征在不同label下对应的权值参数。

    为了更新方便，且易于扩展，这里也采用LTPCWS的方式组织W。

    在LTPCWS中，抽取的特征有：

    1. 当前字，左第一个字，左第二个字，右第一个字，右第二个字 

        W_c , W_l1 , W_l2 , W_r1 , W_r2

    2. 当前字类别，左第一个字类别，右第一个字类别

        类别是指，这个字是中文字符、英文字母、（标点符号）、URL等

        T_c , T_l1 , T_r1

    3. 词典信息

        以该字开始的词语的最大长度

        以改字结尾的词语的最大长度

        过该字的词语的最大长度

        单字长度均为0

        L_start , L_end , L_pass

## Devlopment Log

####20150913

在《自然语言处理》课程给的人民日报语料上评测，结果与LTPCWS对比，如下：

| 方法 | 最佳迭代轮次 |P    | R   | F_1 | F_1 delta |
|------|--------------|-----|-----|-----|-------|
|LTPCWS| 4/5          |93.30|93.31|93.30| 0     |
|self  | 4/5          |87.75|87.75|87.75| 5.55  |
|sekf+bigram+trigram| 5/5| 89.62 | 89.31 | 89.46 | 3.84 |
|self+bigram+FIX_LEXICON| 5/5| 91.15 | 91.36 | 91.26 | 2.04 |

~~F_1值差5.55个点。差距还是比较大的，而原因，注定又将成为一个悬念。~~

~~给这种问题找BUG实在太难了。~~

~~暂时接受该结果，下一个需要实现的应该是POSTAG ， 这个实现理论上与CWS原理相似。一个月的时间。~~

update : 少磊师兄提醒，加入词的bigram、trigram信息。于是按照邓知龙师兄论文中的规则加入了更多词特征，效果见表。有所提升，但是距离LTP仍然差距很大。经过少磊师兄的提醒，我决定搞清楚问题原因！！

update @20150919 : 决心一步一步找问题，这次力求跟LTP保持尽可能的一致性。
1. 保证特征一致性

    unigram 5 个特征

    bigram 4 个特征

    type 3 个特征

    lexicon 3 个特征

    其中在检查lexicon特征时发现了问题，故首先进行了内部词典构建的检查。

    其次type特征也不一致，这次希望改为一致。


2. 构建内部词典(build inner lexicon)

    重新看了LTP中相关代码，发现了一个重大问题——LTP中从所有原始训练数据中提取词构建内部词典时，有两个条件

    1. 出现频率达到一定的阈值

    2. 词长度大于1

    两条的实现都有问题！！

    第一条，LTPCWS实现时，限制阈值取值为保证所取的词的频率和占总词频的90% ， 凡是频率**大于等于**该值的都加入到内部词典中。但是之前实现时，取的是大于该频值，这导致了很多词语的丢失。

    第二条，之前完全没有考虑到！这导致加入了很多单字到inner lexicon中，由于阈值的限制，这间接是inner lexicon中包含更少的真正的词语。（经过这一步，回看上面的阈值取法，可能有些问题。因为上面的频次是全部的频次，没有去处单字的情况。当然，由于前面阈值取90%，又要大于等于该阈值，由于长尾效应，我猜测其实真正能够过滤的词很少。所以，真正起关键作用的可能是第二条。当然，仍然是猜测，需要事实评估。我会在后续补充实际情况。）

    修改时候，效果见表格`self+bigram+FIX_LEXICON`

3. 修改Type特征

    
####20150906

前几天接连有9.3胜利日阅兵，9.4-9.5实验室15周年庆祝活动。时间不是很多，或者没写代码，或者只写了代码没写文档。

到目前为止主体构架基本完成。

这几天主要完成了训练模块中的对开发集的预测功能，预测模块，评价模块。当然这些东西之间都有一些交集。

具体的子模块，主要包含**分词的效果评测** ， 模型保存加载 , 参数解析模块。

**分词效果评测**

指标是常规的p,r,f

基本单位是分出的词。准确率（P）定义为*分词正确的词*在所有*预测分词数*的比例。召回率（R）定义为*分词正确的词*在所有*黄金分词数（gold）*的比例。

基本的处理单位是一个句子。 依次处理每一条句子，比较该句的预测分词结果与黄金分句结果（gold），记录分词正确数，预测分词数，黄金分词数；将每条句子的记录数据相加，最后求取p、r、f。

故最核心问题是**如何获得分词正确的词的数量** 。

开始觉得比较难做，觉得这是一个复杂版的字符串匹配问题。想到分出的词的长度是未知的，一度觉得毫无头绪。

看了LTP中的代码，一下子觉得原来是这样！后来网上又搜索了一下，觉得更加清晰了。

不过依然不能很好的阐述这个问题。首先要说的是，这个匹配是不如字符串匹配的，这是一个**有限制的匹配问题！**

限制：

1. 预测与gold，不管分出的词数量是多少，它们包含的**字数量一定是相等的。**

2. 正确处理的含义是指**该位置的字**被正确分词，即要算分词正确，必然在预测与gold中 两个待比较的词 中的 *第一个字的下标* 是相同的。有点说得不清晰，这里主要强调分词的比较有**字的下标**与**词的下标**这两个维度！！**分词要正确，词下标可能不同，但字下标必然是相同的**，不然位置都不同，正确就无从谈起。

由2，故比较两个序列中的词时，首先的工作就是**对齐两个序列**（对齐**字下标**）！！具体做法，就是使得待比较的两个词，其首字的下标相同。而且，如果已对齐的两个词不匹配，那么这两个词不可能再与其后续词匹配！（因为后续词再也不会与其首字下标相同了）故这时词下标都要向后以后一位。

更加清晰的做法，可以这样（参考了文章[中文分词器分词效果的评测方法](http://www.codelast.com/?p=7042)）：

1. 我们把句子的分词全部表示为二维坐标形式，坐标第一维为词的首字下标，词的第二维为词尾字的下标。若为单字，则坐标第一维与第二维相同。
    
        我      是      中国    人      。
        (0,0)  (1,1)    (2,3)   (4,4)   (5,5)

    经过这样处理所有词都是独一无二的。消除了不同位置的词下标但词本身相同的情况。

2. 当我们将词表示为上述形式后，一种非常直观的获取预测与gold分词相同个数的办法就是将两个序列求交，交集的个数就肯定等价于分词正确的个数。

        set(predict_words_index_sequence) ^ set(gold_words_index_sequence)

    当然，这种方法效率是低下的，这时再对这两个下标序列求相同的词个数，应该更加容易了。

    其实本质与词表示是相同的，复杂的不是比较本身，而是词本身的比较容易掩盖字下标在其中的限制，使得思维容易变得混乱。化为下标序列就解决了这种混乱，使得问题更加清晰。

PS：上述说了一堆，依然感觉说的不清晰。或许这并不是一个复杂的问题，只是由于自身的思维误区才会感觉找不到头绪。我想这就是“聪明”在这个时刻起的作用吧。从复杂的实际问题中抽象出清晰的本质，天才如是也。笨鸟只能细细领悟，但求下次能触类旁通。

**模型保存加载**

首先，由于Extractor需要从训练集中抽取的lexicon，故需要把lexicon保存起来。

其次，就是保存模型Model的一些参数。包含Emit Feature Space ， 就是从训练集中抽的特征（其实是字典，保存了特征到Emit Feature索引的映射，故实际保存了更多的信息），保存了space ， 可以通过length操作获得特征的数量，故数量不必保存。其次是Label Space ，与Emit Feature Space同理。有了这个Space，就能构造（特征，标签）到特征向量的下标转换矩阵了，故转换矩阵不必保存（当然，其实转换矩阵本身就不是必须的，仅仅只是为了cache起来避免重复运算。不过时间效率是否有提升其实是未知的，因为增加了矩阵寻址时间。而且空间开销大大增加了。所以这个转换不一定是OK的。需要具体测试）。其次保存权值Weight相关的信息，当前的W肯定需要保存，W_sum也需要，W_size需要存起来（因为W都是稀疏矩阵，当然，size也完全可以根据space大小计算出来。故不是必须的，但这里还是直接存起来了）。W_time不需要存起来，因为最后保存之前，都做了flush操作，所有时间都是time_now ， 故需要保存time_now， 这样就能完全重建W_time .由此权值相关的信息也可以在加载时恢复了。 

保存采用了pickle + gzip的方式。具体使用方法是：

        import gzip
        import pickle (cPickle)
        
        # save
        zfo = gzip.open("path/to/model" , "wb" )
        pickle.dump(*** , zfo)

        #load
        zfi = gzip.open("path/to/model" , "rb")
        *** = pickle.load(zfi)

**参数解析**

argparse模块非常强大。

由于需要同时处理train , predict , evaluate ，故开始考虑的是分组参数。后来看到argparse支持子parser，且支持绑定对应函数，实在巧妙。

不过只写过这么一次，记不太清楚，这里把代码拷贝过来一点。
        
        argp = argparse.ArgumentParser(description="averaged structured perceptron")
        #! 先构建 子parser组~
        sub_argps = argp.add_subparsers(title="model train" , description="From training dataset to train an averaged structured perceptron model")
        #! 子parser之前也可以为主parser添加参数，这些参数是所有子parser共有的。
        #! 由刚刚构建的子parser组再来构建子parser
        train_argp = sub_argps.add_parser("train")
        train_argp.add_argument("--training-file" , "-train" , help="training dataset for segmentation" , type=str, required=True)
        ...
        #! 设置默认执行的函数
        train_argp.set_defaults(func=seg_train)
        
        args = argp.parse_args()
        #! 高级!!这里很巧妙的调用。前面保存了函数指针，这里直接函数指针+参数体。有各自函数在内部自己处理需要的参数，非常好的思想。
        args.func(args)


####20150831

完成model模块，解码模块，确定解码限制模块方案。

**解码限制模块**

1. 实现接口，输入当前解码位置，返回合法的候选标签

    场景： 

    1. 在位置0，合法标签只有B、S 
        
        从实际考虑，这条规则应该可以被分类器学习到。

    2. 在predict时，如果输入实例本身包含有分词标记，则可以构建一个结构体，保存在分词标记位置的合法标签为{ B , S }

        解决了输入是部分分句的情况，或者有英文单词的情况。

        但考虑该场景意义可能不大。

2. 实现接口，输入当前位置和解码标签，返回前面位置的候选标签

    规则：

    1. 标签{B , S} 的前面只能是 {E , S}

    2. 标签{M , E} 的前面只能是 {B , M}

    同样应该能够被分类器学习到。但理论上能减少计算开销。（减少一半？）

总结以上，可以认为是加入了人为规则。

**解码模块**

解码时，在比较前面标签到该位置该标签的分数大小时，只考虑了前面标签分数和前面标签转移到该标签的分数和，找到最大转移的标签后，再加上发射特征对应的分数。减少了计算量。且将向量分割为转移向量和发射向量。

记录了解码时的实例向量，且直接返回预测标签对应的实例向量序列。减少计算。

####20150830

昨天在写Extractor中确定Type特征时，突然发现：如果不对英文做特殊处理，那么在Extractor这里，每个字母成为了一个单独的元素。这破坏了原始的结构。

这一发现说明了在LTP代码中对英文做特殊处理的原因！需要对英文单词做特殊处理，使得整个单词作为一个Term ， 而不是同汉字一般，每个unicode对应一个字符。

解决方法上，想了两个：

最初就是想在Extractor这里判断字母并合并，如果这样处理，则多处都需要做判断。

后来想到的便是在原始的以unicode字符为单位的基础上做一个类封装，每个元素用一个类实例表示。对英文来说，就是一个单词对应一个实例，对中文来说，就是一个字。

当然后者是更加好的方法。

但是后来又想偏了，出去英文之外，又想到了对连续的数字做封装，成为"长度为1的数字" ， "长度为2的数字" 等，又想到对标点进行归一表示。直到我突然发现，我是在做结构化感知器，不是在做HMM！这些全部都不是必须的。对标点的归一化表示，其实特征模板中的Type就已经完成了该功能！对数字的归一化，结合前后字符的特征模板，同样有该效果！故这都是不必的！在这里纠结了一些时间，还好后来跳出了怪圈，回到了最初的目标--对英文单词做封装。

封装之后改了一些处理代码，总的来说更加复杂了，主要是封装得不太好。其次效率下降了很多。盲测读入instance耗时严重增加！

