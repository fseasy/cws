##基于结构化平均感知器的中文分词

实现结构化感知器

最近因为准备接手师兄的个性化分词模块，同时希望在开学之前能多学一些东西，于是开始在感知器模型上做一些实事。

一件事是在博客上写的关于感知器的[文章](http://memeda.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2015/07/22/%E6%84%9F%E7%9F%A5%E5%99%A8%E5%AD%A6%E4%B9%A0.html)，主要包括感知器模型介绍，目标函数，更新算法。继而引出了一些感知器的变体，包括平均感知器，结构化感知器。*目前仍处于编辑状态。*

另一件事就是写出实际可用的代码。
**目标是快速开发，结构清晰。**

1. 选择Python实现

2. 不要求使用不熟悉的知识，不要求良好封装

3. 效果好

主要从[LTPCWS](https://github.com/HIT-SCIR/ltp-cws)中学习到相关处理细节：

1. 需要从训练集中提取到lexicon信息，并建立词典。

2. 中英文单词，URL，（数字）需要预处理 [暂不考虑]

3. 抽哪些特征
    
    实际抽的特征都是在输入词上抽的，与具体的标签无关。故可以认为这是emit特征（发射特征）。而转移特征可以认为是直接人为赋予的，如果是一阶马尔科夫就是前一个Label到当前label，如果是二阶，那么就是考虑前两个。

    对于emit特征，由于是针对输入字的，故与label无关。即对各label而言，emit特征可以认为是相同的；

    对于trans特征，其是在Viterbi解码时生成的。

    权值向量上，对于emit特征，每个类别都有相应的特征值，设emit特征数为N，那么需要的权值参数为N*label_num ；做一阶，则有label_num*label_num个权值参数，二阶有label_num^3个。

    一个问题是，这些权值参数该如何存储？ 是类似多元分类，为(N+label_num)*label_num的矩阵（仅对一阶有效，二阶可能需要把转移权值单独拿出来），还是把所有权值放在一起，作为一个向量？在LTPCWS中，是后者的处理方式，并使用了一个函数来映射同样的emit特征在不同label下对应的权值参数。

    为了更新方便，且易于扩展，这里也采用LTPCWS的方式组织W。

    在LTPCWS中，抽取的特征有：

    1. 当前字，左第一个字，左第二个字，右第一个字，右第二个字 

        W_c , W_l1 , W_l2 , W_r1 , W_r2

    2. 当前字类别，左第一个字类别，右第一个字类别

        类别是指，这个字是中文字符、英文字母、（标点符号）、URL等

        T_c , T_l1 , T_r1

    3. 词典信息

        以该字开始的词语的最大长度

        以改字结尾的词语的最大长度

        过该字的词语的最大长度

        单字长度均为0

        L_start , L_end , L_pass

## Devlopment Log

####20150906

前几天接连有9.3胜利日阅兵，9.4-9.5实验室15周年庆祝活动。时间不是很多，或者没写代码，或者只写了代码没写文档。

到目前为止主体构架基本完成。

这几天主要完成了训练模块中的对开发集的预测功能，预测模块，评价模块。当然这些东西之间都有一些交集。

具体的子模块，主要包含**分词的效果评测** ， 模型保存加载 。

**分词效果评测**

指标是常规的p,r,f

基本单位是分出的词。准确率（P）定义为*分词正确的词*在所有*预测分词数*的比例。召回率（R）定义为*分词正确的词*在所有*黄金分词数（gold）*的比例。

基本的处理单位是一个句子。 依次处理每一条句子，比较该句的预测分词结果与黄金分句结果（gold），记录分词正确数，预测分词数，黄金分词数；将每条句子的记录数据相加，最后求取p、r、f。

故最核心问题是**如何获得分词正确的词的数量** 。

开始觉得比较难做，觉得这是一个复杂版的字符串匹配问题。想到分出的词的长度是未知的，一度觉得毫无头绪。

看了LTP中的代码，一下子觉得原来是这样！后来网上又搜索了一下，觉得更加清晰了。

不过依然不能很好的阐述这个问题。首先要说的是，这个匹配是不如字符串匹配的，这是一个**有限制的匹配问题！**

限制：

1. 预测与gold，不管分出的词数量是多少，它们包含的**字数量一定是相等的。**

2. 正确处理的含义是指**该位置的字**被正确分词，即要算分词正确，必然在预测与gold中 两个待比较的词 中的 *第一个字的下标* 是相同的。有点说得不清晰，这里主要强调分词的比较有**字的下标**与**词的下标**这两个维度！！**分词要正确，词下标可能不同，但字下标必然是相同的**，不然位置都不同，正确就无从谈起。

由2，故比较两个序列中的词时，首先的工作就是**对齐两个序列**（对齐**字下标**）！！具体做法，就是使得待比较的两个词，其首字的下标相同。而且，如果已对齐的两个词不匹配，那么这两个词不可能再与其后续词匹配！（因为后续词再也不会与其首字下标相同了）故这时词下标都要向后以后一位。

更加清晰的做法，可以这样（参考了文章[中文分词器分词效果的评测方法](http://www.codelast.com/?p=7042)）：

1. 我们把句子的分词全部表示为二维坐标形式，坐标第一维为词的首字下标，词的第二维为词尾字的下标。若为单字，则坐标第一维与第二维相同。
    
        我      是      中国    人      。
        (0,0)  (1,1)    (2,3)   (4,4)   (5,5)

    经过这样处理所有词都是独一无二的。消除了不同位置的词下标但词本身相同的情况。

2. 当我们将词表示为上述形式后，一种非常直观的获取预测与gold分词相同个数的办法就是将两个序列求交，交集的个数就肯定等价于分词正确的个数。

        set(predict_words_index_sequence) ^ set(gold_words_index_sequence)

    当然，这种方法效率是低下的，这时再对这两个下标序列求相同的词个数，应该更加容易了。

    其实本质与词表示是相同的，复杂的不是比较本身，而是词本身的比较容易掩盖字下标在其中的限制，使得思维容易变得混乱。化为下标序列就解决了这种混乱，使得问题更加清晰。

PS：上述说了一堆，依然感觉说的不清晰。或许这并不是一个复杂的问题，只是由于自身的思维误区才会感觉找不到头绪。我想这就是“聪明”在这个时刻起的作用吧。从复杂的实际问题中抽象出清晰的本质，天才如是也。笨鸟只能细细领悟，但求下次能触类旁通。

**模型保存加载**

首先，由于Extractor需要从训练集中抽取的lexicon，故需要把lexicon保存起来。

其次，就是保存模型Model的一些参数。包含Emit Feature Space ， 就是从训练集中抽的特征（其实是字典，保存了特征到Emit Feature索引的映射，故实际保存了更多的信息），保存了space ， 可以通过length操作获得特征的数量，故数量不必保存。其次是Label Space ，与Emit Feature Space同理。 


####20150831

完成model模块，解码模块，确定解码限制模块方案。

**解码限制模块**

1. 实现接口，输入当前解码位置，返回合法的候选标签

    场景： 

    1. 在位置0，合法标签只有B、S 
        
        从实际考虑，这条规则应该可以被分类器学习到。

    2. 在predict时，如果输入实例本身包含有分词标记，则可以构建一个结构体，保存在分词标记位置的合法标签为{ B , S }

        解决了输入是部分分句的情况，或者有英文单词的情况。

        但考虑该场景意义可能不大。

2. 实现接口，输入当前位置和解码标签，返回前面位置的候选标签

    规则：

    1. 标签{B , S} 的前面只能是 {E , S}

    2. 标签{M , E} 的前面只能是 {B , M}

    同样应该能够被分类器学习到。但理论上能减少计算开销。（减少一半？）

总结以上，可以认为是加入了人为规则。

**解码模块**

解码时，在比较前面标签到该位置该标签的分数大小时，只考虑了前面标签分数和前面标签转移到该标签的分数和，找到最大转移的标签后，再加上发射特征对应的分数。减少了计算量。且将向量分割为转移向量和发射向量。

记录了解码时的实例向量，且直接返回预测标签对应的实例向量序列。减少计算。

####20150830

昨天在写Extractor中确定Type特征时，突然发现：如果不对英文做特殊处理，那么在Extractor这里，每个字母成为了一个单独的元素。这破坏了原始的结构。

这一发现说明了在LTP代码中对英文做特殊处理的原因！需要对英文单词做特殊处理，使得整个单词作为一个Term ， 而不是同汉字一般，每个unicode对应一个字符。

解决方法上，想了两个：

最初就是想在Extractor这里判断字母并合并，如果这样处理，则多处都需要做判断。

后来想到的便是在原始的以unicode字符为单位的基础上做一个类封装，每个元素用一个类实例表示。对英文来说，就是一个单词对应一个实例，对中文来说，就是一个字。

当然后者是更加好的方法。

但是后来又想偏了，出去英文之外，又想到了对连续的数字做封装，成为"长度为1的数字" ， "长度为2的数字" 等，又想到对标点进行归一表示。直到我突然发现，我是在做结构化感知器，不是在做HMM！这些全部都不是必须的。对标点的归一化表示，其实特征模板中的Type就已经完成了该功能！对数字的归一化，结合前后字符的特征模板，同样有该效果！故这都是不必的！在这里纠结了一些时间，还好后来跳出了怪圈，回到了最初的目标--对英文单词做封装。

封装之后改了一些处理代码，总的来说更加复杂了，主要是封装得不太好。其次效率下降了很多。盲测读入instance耗时严重增加！

