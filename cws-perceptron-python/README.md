##基于结构化平均感知器的中文分词

实现结构化感知器

最近因为准备接手师兄的个性化分词模块，同时希望在开学之前能多学一些东西，于是开始在感知器模型上做一些实事。

一件事是在博客上写的关于感知器的[文章](http://memeda.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2015/07/22/%E6%84%9F%E7%9F%A5%E5%99%A8%E5%AD%A6%E4%B9%A0.html)，主要包括感知器模型介绍，目标函数，更新算法。继而引出了一些感知器的变体，包括平均感知器，结构化感知器。*目前仍处于编辑状态。*

另一件事就是写出实际可用的代码。
**目标是快速开发，结构清晰。**

1. 选择Python实现

2. 不要求使用不熟悉的知识，不要求良好封装

3. 效果好

主要从[LTPCWS](https://github.com/HIT-SCIR/ltp-cws)中学习到相关处理细节：

1. 需要从训练集中提取到lexicon信息，并建立词典。

2. 中英文单词，URL，（数字）需要预处理 [暂不考虑]

3. 抽哪些特征
    
    实际抽的特征都是在输入词上抽的，与具体的标签无关。故可以认为这是emit特征（发射特征）。而转移特征可以认为是直接人为赋予的，如果是一阶马尔科夫就是前一个Label到当前label，如果是二阶，那么就是考虑前两个。

    对于emit特征，由于是针对输入字的，故与label无关。即对各label而言，emit特征可以认为是相同的；

    对于trans特征，其是在Viterbi解码时生成的。

    权值向量上，对于emit特征，每个类别都有相应的特征值，设emit特征数为N，那么需要的权值参数为N*label_num ；做一阶，则有label_num*label_num个权值参数，二阶有label_num^3个。

    一个问题是，这些权值参数该如何存储？ 是类似多元分类，为(N+label_num)*label_num的矩阵（仅对一阶有效，二阶可能需要把转移权值单独拿出来），还是把所有权值放在一起，作为一个向量？在LTPCWS中，是后者的处理方式，并使用了一个函数来映射同样的emit特征在不同label下对应的权值参数。

    为了更新方便，且易于扩展，这里也采用LTPCWS的方式组织W。

    在LTPCWS中，抽取的特征有：

    1. 当前字，左第一个字，左第二个字，右第一个字，右第二个字 

        W_c , W_l1 , W_l2 , W_r1 , W_r2

    2. 当前字类别，左第一个字类别，右第一个字类别

        类别是指，这个字是中文字符、英文字母、（标点符号）、URL等

        T_c , T_l1 , T_r1

    3. 词典信息

        以该字开始的词语的最大长度

        以改字结尾的词语的最大长度

        过该字的词语的最大长度

        单字长度均为0

        L_start , L_end , L_pass

## Devlopment Log

####20150831

完成model模块，解码模块，确定解码限制模块方案。

**解码限制模块**

1. 实现接口，输入当前解码位置，返回合法的候选标签

    场景： 

    1. 在位置0，合法标签只有B、S 
        
        从实际考虑，这条规则应该可以被分类器学习到。

    2. 在predict时，如果输入实例本身包含有分词标记，则可以构建一个结构体，保存在分词标记位置的合法标签为{ B , S }

        解决了输入是部分分句的情况，或者有英文单词的情况。

        但考虑该场景意义可能不大。

2. 实现接口，输入当前位置和解码标签，返回前面位置的候选标签

    规则：

    1. 标签{B , S} 的前面只能是 {E , S}

    2. 标签{M , E} 的前面只能是 {B , M}

    同样应该能够被分类器学习到。但理论上能减少计算开销。（减少一半？）

总结以上，可以认为是加入了人为规则。

**解码模块**

解码时，在比较前面标签到该位置该标签的分数大小时，只考虑了前面标签分数和前面标签转移到该标签的分数和，找到最大转移的标签后，再加上发射特征对应的分数。减少了计算量。且将向量分割为转移向量和发射向量。

记录了解码时的实例向量，且直接返回预测标签对应的实例向量序列。减少计算。

####20150830

昨天在写Extractor中确定Type特征时，突然发现：如果不对英文做特殊处理，那么在Extractor这里，每个字母成为了一个单独的元素。这破坏了原始的结构。

这一发现说明了在LTP代码中对英文做特殊处理的原因！需要对英文单词做特殊处理，使得整个单词作为一个Term ， 而不是同汉字一般，每个unicode对应一个字符。

解决方法上，想了两个：

最初就是想在Extractor这里判断字母并合并，如果这样处理，则多处都需要做判断。

后来想到的便是在原始的以unicode字符为单位的基础上做一个类封装，每个元素用一个类实例表示。对英文来说，就是一个单词对应一个实例，对中文来说，就是一个字。

当然后者是更加好的方法。

但是后来又想偏了，出去英文之外，又想到了对连续的数字做封装，成为"长度为1的数字" ， "长度为2的数字" 等，又想到对标点进行归一表示。直到我突然发现，我是在做结构化感知器，不是在做HMM！这些全部都不是必须的。对标点的归一化表示，其实特征模板中的Type就已经完成了该功能！对数字的归一化，结合前后字符的特征模板，同样有该效果！故这都是不必的！在这里纠结了一些时间，还好后来跳出了怪圈，回到了最初的目标--对英文单词做封装。

封装之后改了一些处理代码，总的来说更加复杂了，主要是封装得不太好。其次效率下降了很多。盲测读入instance耗时严重增加！

